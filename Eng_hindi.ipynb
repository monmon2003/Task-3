{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Embedding, GRU, LSTM, Bidirectional, Dropout, Activation, TimeDistributed, RepeatVector\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561836</th>\n",
       "      <td>स्पष्टीकरण.–जहां इस उपधारा के अधीन हानि और लाभ...</td>\n",
       "      <td>स्पष्टीकरण.–जहां इस उपधारा के अधीन हानि और लाभ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561837</th>\n",
       "      <td>मैंने गौर किया है कि यह न केवल अपने महत्त्वपूर...</td>\n",
       "      <td>है। I note that this is a landmark meeting – n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561838</th>\n",
       "      <td>उन्होंने मेरे समक्ष जो प्रदर्शन किया उसमें से ...</td>\n",
       "      <td>है। In the presentations that they made before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561839</th>\n",
       "      <td>खाद्य और जल सुरक्षा; पर्यावरण की दृष्टि से वहन...</td>\n",
       "      <td>्त है। Issues such as food and water security;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561840</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1561841 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     hindi  \\\n",
       "0          अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
       "1                          एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
       "2                    निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "3                     ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "4        उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
       "...                                                    ...   \n",
       "1561836  स्पष्टीकरण.–जहां इस उपधारा के अधीन हानि और लाभ...   \n",
       "1561837  मैंने गौर किया है कि यह न केवल अपने महत्त्वपूर...   \n",
       "1561838  उन्होंने मेरे समक्ष जो प्रदर्शन किया उसमें से ...   \n",
       "1561839  खाद्य और जल सुरक्षा; पर्यावरण की दृष्टि से वहन...   \n",
       "1561840                                                NaN   \n",
       "\n",
       "                                                   english  \n",
       "0           Give your application an accessibility workout  \n",
       "1                        Accerciser Accessibility Explorer  \n",
       "2           The default plugin layout for the bottom panel  \n",
       "3              The default plugin layout for the top panel  \n",
       "4           A list of plugins that are disabled by default  \n",
       "...                                                    ...  \n",
       "1561836  स्पष्टीकरण.–जहां इस उपधारा के अधीन हानि और लाभ...  \n",
       "1561837  है। I note that this is a landmark meeting – n...  \n",
       "1561838  है। In the presentations that they made before...  \n",
       "1561839  ्त है। Issues such as food and water security;...  \n",
       "1561840                                                NaN  \n",
       "\n",
       "[1561841 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('hindi_english_parallel.csv\\hindi_english_parallel.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = data.iloc[:10000,1].values\n",
    "english_sentences = list(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_sentences = data.iloc[:10000,0].values\n",
    "hindi_sentences = list(hindi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(english_sentences)):\n",
    "    if type(english_sentences[i]) != str:\n",
    "        english_sentences[i] = 'security'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hindi_sentences)):\n",
    "    if type(hindi_sentences[i]) != str:\n",
    "       hindi_sentences[i] = 'सुरक्षा'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(len(hindi_sentences)):\n",
    "    if type(hindi_sentences[i]) != str:\n",
    "        print(hindi_sentences[i], i)\n",
    "        sum += 1\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "सीमांत (बोर्डर) के रंग को हाइलाइट करें\n",
      "Highlight border color\n"
     ]
    }
   ],
   "source": [
    "print(hindi_sentences[7])\n",
    "print(english_sentences[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(english_sentences))\n",
    "print(len(hindi_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34471 English words.\n",
      "2484 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"the\" \"of\" \"_\" \"a\" \"to\" \"Move\" \"~\" \"onto\" \"Remove\" \"s\"\n",
      "\n",
      "40928 hindi words.\n",
      "2351 unique hindi words.\n",
      "10 Most common words in the hindi dataset:\n",
      "\"का\" \"को\" \"के\" \"करें\" \"(_\" \"में\" \"एक\" \"की\" \"पर\" \"ले\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "hindi_words_counter = collections.Counter([word for sentence in hindi_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "\n",
    "print()\n",
    "print('{} hindi words.'.format(len([word for sentence in hindi_sentences for word in sentence.split()])))\n",
    "print('{} unique hindi words.'.format(len(hindi_words_counter)))\n",
    "print('10 Most common words in the hindi dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*hindi_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "Input: The quick brown fox jumps over the lazy dog .\n",
      " Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "Input: By Jove , my quick study of lexicography won a prize .\n",
      " Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "Input: This is a short sentence .\n",
      " Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .'\n",
    "]\n",
    "\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('Input: {}'.format(sent))\n",
    "    print(' Output: {}'.format(token_sent))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 41\n",
      "Max hindi sentence length: 48\n",
      "English vocabulary size: 1558\n",
      "hindi vocabulary size: 2038\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape,1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_hindi_sentences, english_tokenizer, hindi_tokenizer = preprocess(english_sentences, hindi_sentences)\n",
    "\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_hindi_sequence_length = preproc_hindi_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "hindi_vocab_size = len(hindi_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max hindi sentence length:\", max_hindi_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"hindi vocabulary size:\", hindi_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ''.join([index_to_words[prediction] for prediction in np.argmax(logits,1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(preproc_hindi_sentences.shape[0]):\n",
    "    for j in range(preproc_hindi_sentences.shape[1]):\n",
    "        if preproc_hindi_sentences[i][j] > 2037:\n",
    "            preproc_hindi_sentences[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(preproc_english_sentences.shape[0]):\n",
    "    for j in range(preproc_english_sentences.shape[1]):\n",
    "        if preproc_english_sentences[i][j] > 1557:\n",
    "            preproc_english_sentences[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i in range(preproc_hindi_sentences.shape[0]):\n",
    "    for j in range(preproc_hindi_sentences.shape[1]):\n",
    "        if preproc_hindi_sentences[i][j] > 2037:\n",
    "            sum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 48, 256)           398848    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 48, 512)          789504    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 48, 1024)         525312    \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 48, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 48, 2038)         2088950   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,802,614\n",
      "Trainable params: 3,802,614\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "16/16 [==============================] - 457s 27s/step - loss: 2.1628 - accuracy: 0.8486 - val_loss: 0.7332 - val_accuracy: 0.9545\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 373s 23s/step - loss: 1.5083 - accuracy: 0.9064 - val_loss: 0.7332 - val_accuracy: 0.9545\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 408s 26s/step - loss: 1.5083 - accuracy: 0.9064 - val_loss: 0.7332 - val_accuracy: 0.9545\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 437s 27s/step - loss: 1.5083 - accuracy: 0.9064 - val_loss: 0.7332 - val_accuracy: 0.9545\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 370s 23s/step - loss: 1.5083 - accuracy: 0.9064 - val_loss: 0.7332 - val_accuracy: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f0ad111ee0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bidirectional_embed_model(input_shape, output_sequence_length, english_vocab_size, hindi_vocab_size):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(hindi_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(learning_rate),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_hindi_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_hindi_sentences.shape[-2]))\n",
    "\n",
    "# Build the model\n",
    "embed_rnn_model = bidirectional_embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_hindi_sequence_length,\n",
    "    english_vocab_size,\n",
    "    hindi_vocab_size)\n",
    "\n",
    "print(embed_rnn_model.summary())\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_hindi_sentences, batch_size=512, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tmp_x[4000])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton:\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "\n",
      "Correct Translation:\n",
      "['अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें']\n",
      "\n",
      "Original text:\n",
      "['Give your application an accessibility workout']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediciton:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], hindi_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(hindi_sentences[:1])\n",
    "\n",
    "print('\\nOriginal text:')\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: english_to_hindi_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: english_to_hindi_model\\assets\n"
     ]
    }
   ],
   "source": [
    "embed_rnn_model.save('english_to_hindi_model')\n",
    "# Serialize English Tokenizer to JSON\n",
    "with open('english_tokenizer_1.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(english_tokenizer.to_json(), ensure_ascii=False))\n",
    "    \n",
    "# Serialize Hindi Tokenizer to JSON\n",
    "with open('hindi_tokenizer.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(hindi_tokenizer.to_json(), ensure_ascii=False))\n",
    "    \n",
    "# Save max lengths\n",
    "max_hindi_sequence_length_json = max_hindi_sequence_length\n",
    "with open('sequence_length_hindi.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(max_hindi_sequence_length_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nullclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
